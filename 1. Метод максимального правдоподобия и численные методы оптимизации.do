* Потанин Богдан Станиславович
* Микроэконометрика
* Семинар №1
* Метод максимального правдоподобия и численные методы оптимизации
********
* РАЗДЕЛ №0. Численный метод расчета градиента функции
********
* Мануал по испольование мата в стате https://www.schmidheiny.name/teaching/statamata.pdf
* Запрограммируем функцию x1^2+x2^3-x1*x2 при помощи мата
. mata
function my_func(x1, x2)
{
	y = x1 ^ 2 + x2 ^ 3 - x1 * x2
	return(y)
}
end
* Проверим, что данная функция работает, рассчитав её значения в точке (2, 3)
. mata my_func(2 , 3)
* Найдем её частные производные при помощи численных методов дифференцирования в точке (2,3)
* При этом настоящие значения производных в этих точках равны 1 и 25 соответственно
	* Зададим приращение
. mata delta = 0.00001
	* Найдем частную производную по x1 численным методом
. mata d_my_func_d_x1 = (my_func(2 + delta, 3) - my_func(2, 3)) / delta
. mata d_my_func_d_x1
	* Найдем частную производную по x2 численным методом
. mata d_my_func_d_x2 = (my_func(2, 3 + delta) - my_func(2, 3)) / delta
. mata d_my_func_d_x2
	* Используя полученный значения создадим численно рассчитанный градиент
. mata my_func_grad = (d_my_func_d_x1 \ d_my_func_d_x2)
. mata my_func_grad
	* Задания
		* 1. Найдите градиент функции e^(x1^x2/log(sqrt(x3)))) в точке (1,2,3) численным методом
		* 2. Найдите градиент функции (x1*sin(x2*cos(x3)))^x1 в точке (3,2,1) численным методом
********
* РАЗДЕЛ №1. Решение оптимизационных задач при помощи численных методов
********
* Для ознакомления с численными методами оптимизации рекомендуется прочесть 
* https://www.researchgate.net/publication/313867455_Numerical_Optimization_Methods_in_Economics
* Подробный мануал по численной оптимизации в stata
* https://www.stata.com/manuals13/m-5optimize.pdf
*
* Допустим, что нам необходимо найти локальный максимум некоторой функции
* Для начала запрограммируем эту функции, чтобы при введении определенных аргументов она
* возвращала нам определенные значения
* Пример №1: 2ln(x)+3ln(5-x)
* Запрограммируем функцию
* В этой функции обязательно присутствуют следующие параметры
* x - вектор аргументов фукнции
* y - значение функции (обязательно должен быть рассчитан)
* g - градиент функции (не обязательно должен быть рассчитан)
* H - гессиан функции (не обязательно должен быть рассчитан)
. mata
void func_1(todo, x, y, g, H)
{
	y = 2 * log(x) + 3 * log(5 - x)
}
end
* Применим оптимизатор, где 
* S - объект, содержащий постановку оптимизационной задачи
* optimize_init_evaluator - назначает нашу функцию оптимизационной задаче S
* optimize_init_params - назначает точку 1 (можно поставить любую другую)
* в качестве начальной для используемого алгоритма численной оптимизации
* x - сохраняет вектор значений, максимизирующих функцию
. mata
S = optimize_init()
optimize_init_evaluator(S, &func_1())
optimize_init_params(S, 1)
x = optimize(S)
end
*Посмотрим на найденную точку максимума
mata x
* Недостатком большинства численных методов оптимизации является то, что они, как правило,
* находят лишь один локальный, а не глобальный максимум. Причем, то, какой локальный максимум
* будет найден, может зависеть от начальной точки
* Пример №2. (x1+x2+1)^2-5x1^2-10x2^2
* Следует учесть, что все аргументы функции задаются в виде одного вектора x.
* Поэтому обращаться к x1 будем как x[1], а к x2 через x[2].
* Запрограммируем функцию
. mata
void func_2(todo, x, y, g, H)
{
	y = (x[1] + x[2] + 1) ^ 2 - 5 * x[1] ^ 2 - 10 * x[2] ^ 2
}
end
* В качестве начальной точки возьмем (1, 2)
. mata
S = optimize_init()
optimize_init_evaluator(S, &func_2())
optimize_init_params(S, (1,2))
x = optimize(S)
end
*Посмотрим на найденную точку максимума
mata x
	* Задания
		* 1. Найдите максимум функции ln(x) - exp(x)
		* 2. Найдите максимум функции (x-y) ^ (1/2) - x / y ^ (1/3)
		* 3. Эксперементируя с начальной точкой найдите оба локальных максимума (оба же будут глобальными)
		*    функции -(x^2-1)^2-(x^2*y-x-1)^2.
********
* РАЗДЕЛ №2. РМаксимизация функции правдоподобия при помощи численных методов
********
* Запрограммируем функцию для расчета логарифма функции правдоподобия
* нормального распределения при заданной выборке.
* Где
* normalden - функция, рассчитывающая значение функции плотности нормальной
* случайной величины с математическим ожиданием mu и стандартным отклонением sigma
* по отдельности в каждой из точек вектора x.
* sum - функция, возвращающая сумму элементов вектора
. mata
void lnL(todo, x, my_sample, y, g, H)
{
	mu = x[1]
	sigma2 = x[2]
	
	L_vector = normalden(my_sample, mu, sqrt(sigma2))
	
	y = sum(log(L_vector))
}
end
* Симулируем выборку из нормального распределения
mata n = 10000
mata mu = 1
mata sigma2 = 25
mata my_sample = rnormal(n, 1, mu, sqrt(sigma2))
* Для начала найдем оценки mu и sigma используя аналитическую формулу
	* Оценка математического ожидания mu
mata mu_mle_0 = mean(my_sample)
mata mu_mle_0
	* Оценка стандартного отклонения sigma
	* Где 
	* J(A,B,C) - создает матрицу размерности A на B, заполненную значениями C
	* *: - оператор поэлементного перемножения для векторов и для матриц
mata my_sample_minus_mean = my_sample-J(n, 1, mu_mle_0)
mata sigma2_mle_0 = mean(my_sample_minus_mean :* my_sample_minus_mean)
mata sigma2_mle_0
* Теперь воспользуемся численным методом
* В качестве начальной точки возьмем параметры стандартного нормального распределения
. mata
S = optimize_init()
optimize_init_evaluator(S, &lnL())
optimize_init_params(S, (0,1))
optimize_init_argument(S, 1, my_sample)
x = optimize(S)
H = optimize_result_Hessian(S)
end
* Получим ММП оценки
	* Для mu
mata mu_mle_1 = x[1]
mata mu_mle_1
	* Для sigma
mata sigma2_mle_1 = x[2]
mata sigma2_mle_1
* Сравним результаты аналитического и численного методов и убедимся, что
* они практически идентичны
mata (mu_mle_0, mu_mle_0)
mata (sigma2_mle_0, sigma2_mle_1)
* Найдем аналитическую оценку ковариационной матрицы
mata cov_mle_0 = ((sigma2_mle_0) / n, 0 \ 0, 2 * (sigma2_mle_0 ^ 2) / n)
* Найдем оценку ковариационной матрицы используя Гессианный метод
* Где
* H - полученный ранее Гессиан
* luinv - функция, возвращающая обратную матрицу
mata fisher_1 = -1 * H
mata cov_mle_1 = luinv(fisher_1)
* Наконец, найдем истинную ковариационную матрицу
mata cov_true = (sigma2 / n, 0 \ 0, 2 * sigma2 ^ 2 / n)
* Сравним полученные результаты
mata cov_mle_0
mata cov_mle_1
mata cov_true
* Протестируем гипотезу H0:mu=1.05
mata mu_H0 = 1.05
	* Используя аналитические оценки
		* Рассчитаем значение тестовой статистики
mata z_0 = (mu_mle_0 - mu_H0) / sqrt(cov_mle_0[1,1])
		* Вычислим p-value
mata pvalue_0 = 2 * rowmin((normal(z_0), 1-normal(z_0)))
	* Используя численные оценки
		* Рассчитаем значение тестовой статистики
mata z_1 = (mu_mle_1 - mu_H0) / sqrt(cov_mle_1[1,1])
		* Вычислим p-value
mata pvalue_1 = 2 * rowmin((normal(z_1), 1-normal(z_1)))
* Сравним полученные p-value и убедимся, что их различие невелико
mata (pvalue_0, pvalue_1)
* Теперь, используя дельта метод проверим гипотезу H0: sigma*exp(mu)=12
* Для начала найдем распределение этого выражения, обозначив его через v
mata v_H0 = 11
mata v_mle_0 = sigma2_mle_0 * exp(mu_mle_0)
mata v_mle_1 = sigma2_mle_1 * exp(mu_mle_1)
* Найдем значение градиента рассматриваемого выражения
mata v_gradient_0 = (sigma2_mle_0 * exp(mu_mle_0) \ 1 / (2 * sqrt(sigma2_mle_0)) * exp(mu_mle_0))
mata v_gradient_1 = (sigma2_mle_1 * exp(mu_mle_1) \ 1 / (2 * sqrt(sigma2_mle_1)) * exp(mu_mle_1))
* Воспользуемся дельта методом для нахождения ковариационной матрицы v которая,
* в данном случае, будучи одномерной, совпадает с дисперсией
mata v_cov_0 = v_gradient_0' * cov_mle_0 * v_gradient_0
mata v_cov_1 = v_gradient_1' * cov_mle_1 * v_gradient_1
* Осуществим проверку гипотезы
  * Используя аналитические оценки
mata z_0_new = (v_mle_0 - v_H0) / sqrt(v_cov_0)
mata pvalue_0_new = 2 * rowmin((normal(z_0_new), 1-normal(z_0_new)))
  * Используя численные оценки
mata z_1_new = (v_mle_1 - v_H0) / sqrt(v_cov_1)
mata pvalue_1_new = 2 * rowmin((normal(z_1_new), 1-normal(z_1_new)))
 * Сравним полученные p-value и убедимся, что их различие весьма велико
mata (pvalue_0_new, pvalue_1_new)
* Вывод - разница в результатах обусловлена преимущественно разницей в оценках ковариационной
* матрицы при помощи Гессианного и Аналитического методов.
  *ЗАДАНИЯ
  *1. Повторите описанные выше шаги для распределения Пуассона с параметром lambda = 5.
  *2. Повторите шаги, относящиеся к численным методам и Гессианному методу оценивания
  *   ковариационной матрицы оценок в отношении бета распределения (см. функцию rbeta())
  *   с параметрами rshape1 = 2 и rshape2 = 7.
*-------------
*РАЗДЕЛ №3. Приложение к нелинейному по оцениваемым параметрам регрессионному анализу
*--------------
* Для начала установим пакет, позволяющий работать с многомерным нормальным распределением
*ssc install mvtnorm *УБРАТЬ КОММЕНТАРИИ ДЛЯ STATA-15
* Приступим к симуляциям
	* Количество наблюдений
mata n = 10000
	* Вектор математических ожиданий независимых переменных
mata mu_X = (0,0)
	* Ковариационная матрица независимых переменных
mata cov_X = (1, 0.5 \ 0.5, 1)
		* Делаем копию переменной для статы
mat  cov_X = (1, 0.5 \ 0.5, 1)
	* Матрица независимых переменных
*rmvnormal, n(10000) mean(0, 0) sigma(cov_X) *УБРАТЬ КОММЕНТАРИИ ДЛЯ STATA-15
mata X = (rnormal(n, 1, 0, 1), rnormal(n, 1, 0, 1))
	* Достанем полученный результат
*matrix X = r(rmvnormal) *УБРАТЬ КОММЕНТАРИИ ДЛЯ STATA-15
	* Перенесем переменную из статы в мата
*mata X = st_matrix("X") *УБРАТЬ КОММЕНТАРИИ ДЛЯ STATA-15
	* Возьмем модуль
mata X = abs(X)
	* Посмотрим на сгенерированные значения
mata X 
	* Определим число степеней свободы случайной ошибки
mata df = 10
	* Сгенерируем случайную ошибку из распределения стьюдента
mata epsilon = rt(n, 1, df)
	* Назначим значения для оцениваемых параметров и сгенерируем зависимую переменную
mata beta_0 = 0.5                                  
mata beta_1 = 0.75                                 
mata beta_2 = 0.9                                    
mata Y = beta_0 :+ (X[, 1] :^ beta_1) + (beta_2 :^ X[, 2]) :+ epsilon
* Запрограммируем функцию для расчета логарифма функции правдоподобия 
* в соответствии с обозначенным выше процессом генерации данных
* Обратите внимание, что в mata все переменные глобальные, даже внунтри функции
. mata
void lnL_new(todo, x, X, Y, y, g, H)
{
	df_m = x[1]
	beta_0_m = x[2]
	beta_1_m = x[3]
	beta_2_m = x[4]
	
	L_vector = tden(df_m, (Y :- beta_0_m) :- (X[, 1] :^ beta_1_m) :- (beta_2_m :^ X[, 2]))
	
	y = sum(log(L_vector))
}
end
* Теперь воспользуемся численным методом
. mata
S = optimize_init()
optimize_init_evaluator(S, &lnL_new())
optimize_init_params(S, (5, 1, 1, 1))
optimize_init_argument(S, 1, X)
optimize_init_argument(S, 2, Y)
optimize_init_conv_maxiter(S, 1000)
x = optimize(S)
H = optimize_result_Hessian(S)
end
* Посмотрим на полученные значения
mata x
mata df_mle = x[1]
mata beta_0_mle = x[2]
mata beta_1_mle = x[3]
mata beta_2_mle = x[4]
* Сравним оценки с истинными занчениями параметров
mata (x[1], x[2], x[3], x[4] \ df, beta_0, beta_1, beta_2)
* Найдем реализацию оценки ковариационной матрицы
mata cov_mle = (-1) * luinv(H)
* Протестируем гипотезы
	* H0: beta_1 = 0.7
mata beta_1_H0 = 0.7
mata z = (beta_1_mle - beta_1_H0) / sqrt(cov_mle[3,3])
mata pvalue = 2 * rowmin((normal(z), 1-normal(z)))
mata pvalue
* H0: beta_1^beta_0 + beta_0*beta_2 = 1.35
mata v_H0 = 1.35
mata v_mle = beta_1_mle ^ beta_0_mle + beta_0_mle * beta_2_mle
	* Возьмем частные производные по каждому из оцениваемых параметров
mata v_grad = (0 \                                                        
            beta_1_mle ^ beta_0_mle * log(beta_1_mle) + beta_2_mle \     
            beta_0_mle * beta_1_mle ^ (beta_0_mle - 1) \                 
            beta_0_mle)     
	* Считаем реализацию оценки асимптотичесой дисперсии
mata v_as_var = v_grad' * cov_mle * v_grad   
	* Тестируем гипотезу
mata z = (v_mle - v_H0) / sqrt(v_as_var)                                    
mata pvalue = 2 * rowmin((normal(z), 1-normal(z)))  
mata pvalue
* Теперь перейдем к работе с предельными эффектами
* Для начала найдем реализацию оценки предельного эффекта при x1=2
mata x1_point = 2
mata me_mle = beta_1_mle * x1_point ^ (beta_1_mle - 1)
* Найдем реализацию оценки асимптотической дисперсии оценки предельного эффекта
mata me_grad =      (0 \                                  
                    0 \                                  
                    x1_point ^ (beta_1_mle - 1) *       
                    (beta_1_mle * log(x1_point) + 1) \   
                    0)                       
mata me_asy_var = me_grad' * cov_mle * me_grad
* Протестируем гипотезу
mata z = (me_mle - 0) / sqrt(me_asy_var)
mata pvalue = 2 * rowmin((normal(z), 1-normal(z)))       
mata pvalue
*ЗАДАНИЯ
  * 1. Проверьте, как изменится точность оценок, если в функции правдоподобия
  *    использовать не распределение стьюдента, а логистическое распределение
  *    (функция dlogis()), где параметр df задает значение аргумента scale
  *    в функции dlogis (параметр локации логистического распределения). 
  *    При этом подберите приемлимую начальную точку, например, положив x0 = c(1, 1, 1, 1).
  * 2. Проверьте H0:log(beta_0+beta_2)=sqrt(beta_0*beta_1*beta_2)
  * 3. При помощи LR теста проверьте гипотезу beta_0 = beta_1.
  * 4. Оцените предельный эффект для beta_2 и проверьте гипотезу о его равенстве 1.
  * 5. Самостоятельно придумайте модель, запишите её процесс генерации
  *    и попытайтесь оценить параметры модели численными методами.
  